{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "\n",
    "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
    "\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "testfile = urllib.request.URLopener()\n",
    "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
    "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
    "\n",
    "df_train = pd.read_csv('SPECT.train',header=None)\n",
    "df_test = pd.read_csv('SPECT.test',header=None)\n",
    "\n",
    "train = df_train.values\n",
    "test = df_test.values\n",
    "\n",
    "y_train = train[:,0]\n",
    "X_train = train[:,1:]\n",
    "y_test = test[:,0]\n",
    "X_test = test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3509872  -0.05713294 -0.93779756  0.77741797 -0.38695241  1.76093211\n",
      "  0.3947773   1.11434433 -0.08836121  0.37166612 -2.18818974 -2.1206884\n",
      " -0.53060544 -1.0061862  -0.35551717  0.23426643  0.86950982 -0.1660445\n",
      " -0.47112301 -0.60312883 -1.02718706  1.17417996]\n",
      "[-2.3509872  -0.05713294 -0.93779756  0.77741797 -0.38695241  1.76093211\n",
      "  0.3947773   1.11434433 -0.08836121  0.37166612 -2.18818974 -2.1206884\n",
      " -0.53060544 -1.0061862  -0.35551717  0.23426643  0.86950982 -0.1660445\n",
      " -0.47112301 -0.60312883 -1.02718706  1.17417996]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.randn(22)\n",
    "print(w)\n",
    "print(w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
    "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$. \n",
    "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$. \n",
    "To adapt the learning rate the Barzilai-Borwein method is used.\n",
    "\n",
    "Try to understand each step of the learning algorithm and comment each line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_reg_ERM(X,y,lbda):\n",
    "    max_iter = 200\n",
    "    e  = 0.001\n",
    "    alpha = 1.\n",
    "    np.random.seed(5)\n",
    "    w = np.random.randn(X.shape[1]);\n",
    "    for k in np.arange(max_iter):\n",
    "        h = np.dot(X,w) #compute output\n",
    "        l,lg = loss(h, y) # calculate loss and loss gradient\n",
    "        print ('loss: {}'.format(np.mean(l)))\n",
    "        r,rg = reg(w, lbda) # calculate regularisation value\n",
    "        g = np.dot(X.T,lg) + rg # gradient\n",
    "        if (k > 0):\n",
    "            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old)) \n",
    "        w = w - alpha * g # update weight\n",
    "        if (np.linalg.norm(alpha * g) < e):\n",
    "            break\n",
    "        g_old = g\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0490584038582944\n",
      "loss: 5.238635546313249\n",
      "loss: 3.1824805388585107\n",
      "loss: 1.7885229173035568\n",
      "loss: 2.563589821865231\n",
      "loss: 2.67792906133404\n",
      "loss: 1.462462666701748\n",
      "loss: 102.16606489613007\n",
      "loss: 14.11159259334483\n",
      "loss: 37.81235875481328\n",
      "loss: 16.626304920321296\n",
      "loss: 181.9\n",
      "loss: 12.280508703144251\n",
      "loss: 28.787305041756564\n",
      "loss: 8.087761504938573\n",
      "loss: 166.82706189692522\n",
      "loss: 15.769615986153571\n",
      "loss: 29.24896133550312\n",
      "loss: 8.628133991306118\n",
      "loss: 135.3274726403648\n",
      "loss: 12.05378027544898\n",
      "loss: 16.63141339563265\n",
      "loss: 2.3876823561522413\n",
      "loss: 26.358408076692605\n",
      "loss: 1.88089547391029\n",
      "loss: 1.2905003781235487\n",
      "loss: 1.2057564183490712\n",
      "loss: 0.8718097975041823\n",
      "loss: 0.8093685603002762\n",
      "loss: 0.9983498913564244\n",
      "loss: 1.5123676996405784\n",
      "loss: 1.074706623702789\n",
      "loss: 0.7383802160671893\n",
      "loss: 0.6395755498805278\n",
      "loss: 0.7228113435801344\n",
      "loss: 0.6417948973723517\n",
      "loss: 0.6138804476451705\n",
      "loss: 0.6090388018149946\n",
      "loss: 0.6136034926498863\n",
      "loss: 0.7183913384357081\n",
      "loss: 0.5936782463016861\n",
      "loss: 0.5925041647786986\n",
      "loss: 0.5978106950531084\n",
      "loss: 0.5853167411526506\n",
      "loss: 0.5830392607967092\n",
      "loss: 0.5885588571630119\n",
      "loss: 0.5815234822104328\n",
      "loss: 0.5792506797564647\n",
      "loss: 0.5768496155705252\n",
      "loss: 0.5871267855741203\n",
      "loss: 0.6023408782320522\n",
      "loss: 0.6005513883935057\n",
      "loss: 0.5737158305022821\n",
      "loss: 0.5633176941980564\n",
      "loss: 0.57704979420156\n",
      "loss: 0.5706534829420129\n",
      "loss: 0.5653922099482473\n",
      "loss: 0.5567692431701735\n",
      "loss: 0.5529795943513258\n",
      "loss: 0.549714436809094\n",
      "loss: 0.549144163322488\n",
      "loss: 0.5495192439587939\n",
      "loss: 0.5494410509401795\n",
      "loss: 0.545562581018047\n",
      "loss: 0.544420525241857\n",
      "loss: 0.544925564898069\n",
      "loss: 0.5429291703072042\n",
      "loss: 0.5422614642408426\n",
      "loss: 0.5412158213684934\n",
      "loss: 0.5406766763432553\n",
      "loss: 0.5421131229197956\n",
      "loss: 0.5397619051023501\n",
      "loss: 0.5393763225028759\n",
      "loss: 0.5418561613166595\n",
      "loss: 0.5394086988415803\n",
      "loss: 0.5383660930356993\n",
      "loss: 0.5380516985228576\n",
      "loss: 0.5377710559048788\n",
      "loss: 0.5373655804476355\n",
      "loss: 0.5383573131741525\n",
      "loss: 0.5367646141503641\n",
      "loss: 0.536367756660907\n",
      "loss: 0.5365487931091146\n",
      "loss: 0.549293266139925\n",
      "loss: 0.5378774710363349\n",
      "loss: 0.5434943656498991\n",
      "loss: 0.5316666480853366\n",
      "loss: 0.5287672963589307\n",
      "loss: 0.5296186030549939\n",
      "loss: 0.5280795197489268\n",
      "loss: 0.5276245222811025\n",
      "loss: 0.5277336122227241\n",
      "loss: 0.5283827418428031\n",
      "loss: 0.5266579240437798\n",
      "loss: 0.5256735382862553\n",
      "loss: 0.52527181827893\n",
      "loss: 0.5273201126481559\n",
      "loss: 0.52495337180035\n",
      "loss: 0.5248879852947128\n",
      "loss: 0.5247448263809781\n",
      "loss: 0.523780989863867\n",
      "loss: 16.32500000000018\n",
      "loss: 8.521474308079599\n",
      "loss: 4.86060774764529\n",
      "loss: 2.584171776644534\n",
      "loss: 5.314862522736018\n",
      "loss: 2.153392114383891\n",
      "loss: 87.62278929179236\n",
      "loss: 11.4499927639019\n",
      "loss: 16.821257604817\n",
      "loss: 5.089294326665718\n",
      "loss: 132.88406181783247\n",
      "loss: 14.787421617424679\n",
      "loss: 26.188428801221797\n",
      "loss: 7.987525623751398\n",
      "loss: 181.9\n",
      "loss: 18.171281020916776\n",
      "loss: 34.15826591239381\n",
      "loss: 10.67961932068864\n",
      "loss: 181.9\n",
      "loss: 15.873442180268494\n",
      "loss: 35.878944097519046\n",
      "loss: 11.942466898056493\n",
      "loss: 181.90000000000003\n",
      "loss: 14.892976822349112\n",
      "loss: 36.60216942618087\n",
      "loss: 12.475152813937651\n",
      "loss: 181.90000000000003\n",
      "loss: 14.48154475989322\n",
      "loss: 36.9037341357855\n",
      "loss: 12.697610184667482\n",
      "loss: 181.9\n",
      "loss: 14.310098654562148\n",
      "loss: 37.029064895420106\n",
      "loss: 12.790123865227267\n",
      "loss: 181.9\n",
      "loss: 14.238863803290934\n",
      "loss: 37.081081658373925\n",
      "loss: 12.828530773932936\n",
      "loss: 181.9\n",
      "loss: 14.209301919937682\n",
      "loss: 37.102658304371516\n",
      "loss: 12.844463824257327\n",
      "loss: 181.9\n",
      "loss: 14.197040135116328\n",
      "loss: 37.1116062428118\n",
      "loss: 12.851071645778921\n",
      "loss: 181.89999999999995\n",
      "loss: 14.191955206408597\n",
      "loss: 37.11531663599887\n",
      "loss: 12.853811728549848\n",
      "loss: 181.89999999999998\n",
      "loss: 14.18984668239068\n",
      "loss: 37.11685514280897\n",
      "loss: 12.854947907314013\n",
      "loss: 181.89999999999995\n",
      "loss: 14.188972390018753\n",
      "loss: 37.117493070822526\n",
      "loss: 12.855419015194233\n",
      "loss: 181.89999999999998\n",
      "loss: 14.188609873016892\n",
      "loss: 37.1177575801088\n",
      "loss: 12.855614354760803\n",
      "loss: 181.89999999999995\n",
      "loss: 14.188459559721263\n",
      "loss: 37.11786725543353\n",
      "loss: 12.855695349810157\n",
      "loss: 181.89999999999995\n",
      "loss: 14.188397234288066\n",
      "loss: 37.117912730822084\n",
      "loss: 12.855728933318801\n",
      "loss: 181.9\n",
      "loss: 14.188371391893401\n",
      "loss: 37.11793158656697\n",
      "loss: 12.855742858260488\n",
      "loss: 181.9\n",
      "loss: 14.188360676699991\n",
      "loss: 37.11793940484087\n",
      "loss: 12.855748632045252\n",
      "loss: 181.89999999999995\n",
      "loss: 14.188356233793249\n",
      "loss: 37.11794264657977\n",
      "loss: 12.855751026065116\n",
      "loss: 181.9\n",
      "loss: 14.188354391603564\n",
      "loss: 37.11794399072183\n",
      "loss: 12.855752018712248\n",
      "loss: 181.9\n",
      "loss: 14.188353627765178\n",
      "loss: 37.11794454805167\n",
      "loss: 12.855752430299594\n",
      "loss: 181.9\n",
      "loss: 14.18835331105023\n",
      "loss: 37.11794477914073\n",
      "loss: 12.855752600958562\n",
      "loss: 181.9\n",
      "loss: 14.18835317972879\n",
      "loss: 37.11794487495858\n",
      "loss: 12.855752671719912\n",
      "loss: 181.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-36.63403679,   2.88819041,  -6.87292319,   3.00763361,\n",
       "       -39.28334079,  -4.701392  ,   3.12707681,  15.65693761,\n",
       "        -2.05208799, -21.81320159,  10.47777281,   0.59721601,\n",
       "        20.83610241,   0.47777281,   5.1791648 ,  25.41805121,\n",
       "        20.2388864 ,  15.1791648 , -12.05208799,  -4.46250559,\n",
       "         8.06735521,  -4.22361919])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient. \n",
    "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y):\n",
    "    ##################\n",
    "    #INSERT CODE HERE#\n",
    "    ##################\n",
    "    l = np.maximum(0,1-y*h)\n",
    "    g = np.where(l > 0, -y, 0) # or -y * (l > 0)\n",
    "    return l, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, -1, -7, -3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6])\n",
    "b = np.array([3,1,-5,1,7,3])\n",
    "np.where(a > 3, -b,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$. \n",
    "\n",
    "\n",
    "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
    "\n",
    "$$g = \\lambda \\textbf{w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(w, lbda):\n",
    "    ##################\n",
    "    #INSERT CODE HERE#\n",
    "    ##################\n",
    "    r = (lbda/2) * np.dot(w.T,w) * 1.\n",
    "    g = lbda * w\n",
    "    return r, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    ##################\n",
    "    #INSERT CODE HERE#\n",
    "    ##################\n",
    "    preds = np.where(np.dot(X,w) <= 0, 0, 1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.764763226118641\n",
      "loss: 8.438830068223563\n",
      "loss: 5.106389903905269\n",
      "loss: 3.8275622733374646\n",
      "loss: 2.6511614597117035\n",
      "loss: 2.154400875141779\n",
      "loss: 1.1995298694131749\n",
      "loss: 0.989722849226798\n",
      "loss: 0.828967299455017\n",
      "loss: 0.7995984535391586\n",
      "loss: 0.7132537764414422\n",
      "loss: 0.6835164020488783\n",
      "loss: 775.7374999999431\n",
      "loss: 1160.424462015542\n",
      "loss: 354.1734901524022\n",
      "loss: 98.0904551963769\n",
      "loss: 297.979592316903\n",
      "loss: 416.4455776001611\n",
      "loss: 149.8751555770516\n",
      "loss: 70.27844741855475\n",
      "loss: 50.092218163472616\n",
      "loss: 43.49420143560234\n",
      "loss: 26.05197375419437\n",
      "loss: 21.818386636004924\n",
      "loss: 63.90881364116818\n",
      "loss: 61.447760938923125\n",
      "loss: 18.240400152261987\n",
      "loss: 15.076991507023132\n",
      "loss: 20.276557831716136\n",
      "loss: 12.929565081914086\n",
      "loss: 11.822391420366454\n",
      "loss: 15.86241727121326\n",
      "loss: 11.094230368392829\n",
      "loss: 10.695375129384626\n",
      "loss: 10.602348091049866\n",
      "loss: 9.858559872059327\n",
      "loss: 9.637135035776302\n",
      "loss: 10.294402599863052\n",
      "loss: 9.75387679107681\n",
      "loss: 9.520951075089624\n",
      "loss: 9.18970125100846\n",
      "loss: 9.083445103291059\n",
      "loss: 8.995699769940472\n",
      "loss: 8.910188134667584\n",
      "loss: 8.771465328906121\n",
      "loss: 8.75231359940939\n",
      "loss: 8.732173953372774\n",
      "loss: 8.694423438504588\n",
      "loss: 1613.2374999999477\n",
      "loss: 608.9132209003787\n",
      "loss: 267.9693326764776\n",
      "loss: 419.4457853555721\n",
      "loss: 225.09738079635986\n",
      "loss: 114.56987517585644\n",
      "loss: 48.40335718133646\n",
      "loss: 63.27652118035051\n",
      "loss: 84.13656584975665\n",
      "loss: 53.26904871737312\n",
      "loss: 32.917417535921935\n",
      "loss: 71.0611896852743\n",
      "loss: 25.378689021494406\n",
      "loss: 14.706657418019045\n",
      "loss: 12.190816280792617\n",
      "loss: 1988.2000000000182\n",
      "loss: 1557.947859687622\n",
      "loss: 711.5438660538099\n",
      "loss: 4387.016980998227\n",
      "loss: 278.13581758166686\n",
      "loss: 158.5788063072992\n",
      "loss: 208.89058303775514\n",
      "loss: 65.87000788920788\n",
      "loss: 66.71770459152712\n",
      "loss: 35.819598842833976\n",
      "loss: 32.36640125935443\n",
      "loss: 26.230508058322823\n",
      "loss: 26.082821942576526\n",
      "loss: 22.085512025066805\n",
      "loss: 20.07785978939839\n",
      "loss: 18.120976924602434\n",
      "loss: 26.609176333073883\n",
      "loss: 24.42496202414427\n",
      "loss: 17.72892771061232\n",
      "loss: 16.337907839211567\n",
      "loss: 25.26949978044535\n",
      "loss: 17.27176951806115\n",
      "loss: 14.203937644643778\n",
      "loss: 13.595035835913759\n",
      "loss: 13.789283530217887\n",
      "loss: 12.60064363809499\n",
      "loss: 12.280597079221257\n",
      "loss: 12.698935994120784\n",
      "loss: 12.336232898338707\n",
      "loss: 11.779006826507\n",
      "loss: 11.640518995760067\n",
      "loss: 11.545869516146391\n",
      "loss: 11.48091509898747\n",
      "loss: 11.663427726146725\n",
      "loss: 11.16358610410102\n",
      "loss: 10.905914080594306\n",
      "loss: 10.819704986967412\n",
      "loss: 10.949242939143897\n",
      "loss: 10.774182780579295\n",
      "loss: 10.720946983207247\n",
      "loss: 10.4023968341784\n",
      "loss: 10.33857708070916\n",
      "loss: 10.224781925749836\n",
      "loss: 10.378758071263231\n",
      "loss: 10.087004680289429\n",
      "loss: 10.069424373925717\n",
      "loss: 9.924027651737475\n",
      "loss: 9.87722714805453\n",
      "loss: 9.863790750695816\n",
      "loss: 9.84591638863355\n",
      "loss: 9.785304610510433\n",
      "loss: 9.777355969174199\n",
      "loss: 9.756067583575987\n",
      "loss: 9.740391109546568\n",
      "loss: 9.712691845325825\n",
      "loss: 9.723581968669102\n",
      "loss: 9.719876722241958\n",
      "loss: 9.660601667299597\n",
      "loss: 9.639863008963816\n",
      "loss: 9.646513043842225\n",
      "loss: 9.618392111913543\n",
      "loss: 9.594020347934459\n",
      "loss: 9.660307708650834\n",
      "loss: 9.564445314660036\n",
      "loss: 9.540605978701269\n",
      "loss: 9.531914551538645\n",
      "loss: 9.519698841770083\n",
      "loss: 9.504013579587788\n",
      "loss: 9.499984026604546\n",
      "loss: 9.479973821305752\n",
      "loss: 9.450036361093266\n",
      "loss: 10.041949747240878\n",
      "loss: 9.798172607783261\n",
      "loss: 9.4702025351282\n",
      "loss: 10.55880854734138\n",
      "loss: 9.384415842333249\n",
      "loss: 9.176816375646053\n",
      "loss: 9.095043399281646\n",
      "loss: 9.18535181772653\n",
      "loss: 9.043968854010412\n",
      "loss: 9.017467371329774\n",
      "loss: 8.985249751205016\n",
      "loss: 8.965468206597993\n",
      "loss: 8.905107019442818\n",
      "loss: 8.927852985865567\n",
      "loss: 8.906451332712301\n",
      "loss: 8.841773092921065\n",
      "loss: 8.853780218940631\n",
      "loss: 8.794005002997695\n",
      "loss: 8.743741900243137\n",
      "loss: 8.725643914270975\n",
      "loss: 8.892451239448063\n",
      "loss: 8.842190749849033\n",
      "loss: 8.717334929096445\n",
      "loss: 9.858288838667278\n",
      "loss: 8.606212535674574\n",
      "loss: 8.553393420319287\n",
      "loss: 8.630451989260902\n",
      "loss: 8.582536719929006\n",
      "loss: 8.474665557684872\n",
      "loss: 8.512234869646125\n",
      "loss: 8.440712856647332\n",
      "loss: 8.417520627400359\n",
      "loss: 8.542487759173522\n",
      "loss: 8.48494226802649\n",
      "loss: 8.368523892738274\n",
      "loss: 8.443601310479778\n",
      "loss: 8.345113549210424\n",
      "loss: 8.304142429913178\n",
      "loss: 8.289456105179815\n",
      "loss: 8.276057621621224\n",
      "loss: 8.260398875798057\n",
      "loss: 8.242786261107891\n",
      "loss: 8.221754979712827\n",
      "loss: 8.218649713594507\n",
      "loss: 8.211675879795589\n",
      "loss: 8.196940994903052\n",
      "loss: 8.192492353545068\n",
      "loss: 8.186812331920057\n",
      "loss: 8.193063511264956\n",
      "loss: 8.175893561872902\n",
      "loss: 8.177978780100666\n",
      "loss: 8.18482646966182\n",
      "loss: 8.16294462519927\n",
      "loss: 8.158586343590684\n",
      "loss: 8.154759103796698\n",
      "loss: 8.148168422510645\n",
      "loss: 8.139761275057115\n",
      "loss: 8.138956047612483\n",
      "loss: 8.131416364589745\n",
      "loss: 8.129345219909778\n",
      "loss: 8.126645781734101\n",
      "loss: 8.12239788466307\n",
      "loss: 8.124991845385209\n",
      "loss: 8.120041662302173\n",
      "loss: 8.115033711450328\n",
      "loss: 8.115008549444033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6631016042780749"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = learn_reg_ERM(X_train,np.where(y_train == 0, -1, y_train),0.0001)\n",
    "np.sum(predict(w, X_test)== y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "#### 5.1 \n",
    "Train a linear model on the training data and classify all 187 test instances afterwards using the function predict. \n",
    "Please note that the given class labels are in the range $\\left \\{0,1 \\right \\}$, however the learning algorithm expects a label in the range of $\\left \\{-1,1 \\right \\}$. Then, compute the accuracy of your trained linear model on both the training and the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.4738741465975802\n",
      "loss: 10.636001034611189\n",
      "loss: 5.635684817740274\n",
      "loss: 181250.650000003\n",
      "loss: 24042.200572618945\n",
      "loss: 60106.853442870524\n",
      "loss: 23824.006906191375\n",
      "loss: 181250.65\n",
      "loss: 8827.493392650344\n",
      "loss: 6283.224231197714\n",
      "loss: 11675.963806485835\n",
      "loss: 2413.4054427935644\n",
      "loss: 1239.6463369488465\n",
      "loss: 4823.637694653042\n",
      "loss: 1600.6176065512923\n",
      "loss: 652.146922998109\n",
      "loss: 584.370887901542\n",
      "loss: 476.0863186318549\n",
      "loss: 380.8800379710348\n",
      "loss: 387.73280479322455\n",
      "loss: 399.27653955226253\n",
      "loss: 320.83304187851604\n",
      "loss: 294.65684938140976\n",
      "loss: 252.24889254485387\n",
      "loss: 283.6503403646302\n",
      "loss: 367.7303128047353\n",
      "loss: 231.20194392204561\n",
      "loss: 207.781659445481\n",
      "loss: 201.8633271442271\n",
      "loss: 189.6908611747104\n",
      "loss: 194.0224991142582\n",
      "loss: 181.33675190171624\n",
      "loss: 180.76947759114137\n",
      "loss: 174.801111931447\n",
      "loss: 170.85182050899874\n",
      "loss: 171.624829682261\n",
      "loss: 168.42288103740484\n",
      "loss: 6875.712500000173\n",
      "loss: 5689.898372628648\n",
      "loss: 2874.752066487426\n",
      "loss: 2933.8145038617986\n",
      "loss: 1049.8776440823451\n",
      "loss: 419.42540837946433\n",
      "loss: 1192.1375808889684\n",
      "loss: 1164.9359297769358\n",
      "loss: 429.90968746428626\n",
      "loss: 165.15756284548857\n",
      "loss: 145.263489358657\n",
      "loss: 175.20608335017874\n",
      "loss: 360.5415232862142\n",
      "loss: 130.97620434253903\n",
      "loss: 100.05098709670733\n",
      "loss: 92.3569251698838\n",
      "loss: 87.28828265784526\n",
      "loss: 90.03784628261789\n",
      "loss: 79.63506069799533\n",
      "loss: 77.21054123773033\n",
      "loss: 78.21216148504746\n",
      "loss: 73.93531726873174\n",
      "loss: 74.37260265022273\n",
      "loss: 71.69448950759059\n",
      "loss: 70.07318685695243\n",
      "loss: 5875.787499999751\n",
      "loss: 14373.529894423864\n",
      "loss: 5670.730274863317\n",
      "loss: 34278.572142394856\n",
      "loss: 1987.5887729608999\n",
      "loss: 834.2725592332579\n",
      "loss: 991.1869803916059\n",
      "loss: 667.3374409042814\n",
      "loss: 435.77057595507887\n",
      "loss: 373.1240451303632\n",
      "loss: 27250.749999999964\n",
      "loss: 12308.422568325039\n",
      "loss: 4857.7300755864035\n",
      "loss: 95473.99842649519\n",
      "loss: 9210.63275952192\n",
      "loss: 6457.101860837854\n",
      "loss: 4550.461187969227\n",
      "loss: 2798.5394252049773\n",
      "loss: 1430.5168604706837\n",
      "loss: 1505.0844678006017\n",
      "loss: 1580.2161728552812\n",
      "loss: 690.7415265846778\n",
      "loss: 509.73856740848913\n",
      "loss: 418.6983712465907\n",
      "loss: 919.8083077221556\n",
      "loss: 708.42868911526\n",
      "loss: 263.8790752489922\n",
      "loss: 227.83430296453935\n",
      "loss: 1585.8851290790192\n",
      "loss: 1754.8045100800068\n",
      "loss: 397.84917302210056\n",
      "loss: 13853.180060857121\n",
      "loss: 1218.6000169785816\n",
      "loss: 2330.7059083317627\n",
      "loss: 739.0555727868673\n",
      "loss: 490.88825816336487\n",
      "loss: 506.2827208985981\n",
      "loss: 257.4300630917618\n",
      "loss: 221.9432961832552\n",
      "loss: 179.17632033585764\n",
      "loss: 461.79233948381\n",
      "loss: 146.90141290593706\n",
      "loss: 123.95338141826733\n",
      "loss: 155.00859082272274\n",
      "loss: 110.53718024735869\n",
      "loss: 106.18156515127207\n",
      "loss: 93.40318036025826\n",
      "loss: 32250.775000000158\n",
      "loss: 7824.523256079922\n",
      "loss: 2875.3153849344135\n",
      "loss: 2539.5779732313044\n",
      "loss: 3172.3596302351757\n",
      "loss: 1072.8734904259854\n",
      "loss: 635.5245564066402\n",
      "loss: 318.4033946359927\n",
      "loss: 714.4974410597688\n",
      "loss: 308.63626246394176\n",
      "loss: 202.48046459936123\n",
      "loss: 177.01335869478538\n",
      "loss: 207.57106778906012\n",
      "loss: 144.22338201336953\n",
      "loss: 132.54908565332718\n",
      "loss: 116.84527048690384\n",
      "loss: 154.45356703214708\n",
      "loss: 109.21885391640451\n",
      "loss: 98.17742307962901\n",
      "loss: 91.44518041578183\n",
      "loss: 88.61827154050297\n",
      "loss: 85.52477613297324\n",
      "loss: 84.28418521080432\n",
      "loss: 83.59499787739338\n",
      "loss: 81.84513426820911\n",
      "loss: 82.37840949198548\n",
      "loss: 80.25205020766221\n",
      "loss: 79.0705265086365\n",
      "loss: 79.23223014286128\n",
      "loss: 78.43700747036056\n",
      "loss: 78.04981887263389\n",
      "loss: 77.03887254807894\n",
      "loss: 80.47495004409389\n",
      "loss: 81.12760925869485\n",
      "loss: 74.71497479429867\n",
      "loss: 75.40295911265177\n",
      "loss: 73.60929913020185\n",
      "loss: 73.06820739978481\n",
      "loss: 72.84004279720484\n",
      "loss: 72.43807819424994\n",
      "loss: 72.34304326810363\n",
      "loss: 71.83438413788028\n",
      "loss: 71.40393712460623\n",
      "loss: 70.87655538011175\n",
      "loss: 82.02713175573945\n",
      "loss: 70.6691134120931\n",
      "loss: 70.02143574346988\n",
      "loss: 71.34945877359641\n",
      "loss: 69.12227592276606\n",
      "loss: 68.77502281205665\n",
      "loss: 68.63065647662917\n",
      "loss: 68.44624179397917\n",
      "loss: 5625.712500000213\n",
      "loss: 3626.8879482754296\n",
      "loss: 3257.918052900812\n",
      "loss: 2821.2574814317522\n",
      "loss: 914.9865064822691\n",
      "loss: 3999.006917122191\n",
      "loss: 443.36491439037474\n",
      "loss: 235.740160941043\n",
      "loss: 196.96740581850668\n",
      "loss: 163.5169438973115\n",
      "loss: 222.6001251477669\n",
      "loss: 390.59691296425433\n",
      "loss: 132.48781523549692\n",
      "loss: 157.09473613785914\n",
      "loss: 99.24967693815351\n",
      "loss: 87.441648200655\n",
      "loss: 95.09424016446012\n",
      "loss: 89.16948816080163\n",
      "loss: 72.10647086377007\n",
      "loss: 67.48469520763845\n",
      "loss: 64.13717640113227\n",
      "loss: 62.37085253302082\n",
      "loss: 62.13986878085213\n",
      "loss: 58.66025548320461\n",
      "loss: 56.69563632390866\n",
      "loss: 55.547791958110906\n",
      "loss: 54.65032442299632\n",
      "loss: 53.3635243653077\n",
      "loss: 53.37427453081666\n",
      "loss: 51.511878893173005\n",
      "loss: 51.38480792515198\n",
      "loss: 50.94364694999179\n",
      "loss: 50.66945597359162\n",
      "loss: 51.080668787799596\n",
      "loss: 50.33386799418286\n",
      "loss: 54.370349616930206\n",
      "loss: 50.366831541523446\n",
      "loss: 52.92532644214576\n",
      "loss: 49.54802790781227\n",
      "Accuracy on test_data =  73.2620320855615\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#INSERT CODE HERE#\n",
    "##################\n",
    "y_train = np.where(y_train == 0, -1, y_train)\n",
    "lbda = 0.0001\n",
    "model = learn_reg_ERM(X_train,y_train,lbda)\n",
    "preds = predict(model,X_test)\n",
    "accuracy = (np.sum(preds == y_test)*100)/len(y_test)\n",
    "print('Accuracy on test_data = ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2\n",
    "Compare the accuracy of the linear model with the accuracy of a random forest and a decision tree on the training and test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test_data using Random Forest =  68.98395721925134\n",
      "Accuracy on test_data using Decision Tree =  60.42780748663102\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#INSERT CODE HERE#\n",
    "##################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "accuracy = (np.sum(preds == y_test)*100)/len(y_test)\n",
    "print('Accuracy on test_data using Random Forest = ',accuracy)\n",
    "\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree.fit(X_train, y_train)\n",
    "preds = clf_tree.predict(X_test)\n",
    "accuracy = (np.sum(preds == y_test)*100)/len(y_test)\n",
    "print('Accuracy on test_data using Decision Tree = ',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
